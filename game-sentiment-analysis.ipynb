{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef2ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stopwords\n",
      "  Downloading stopwords-1.0.2-py2.py3-none-any.whl (37 kB)\n",
      "Installing collected packages: stopwords\n",
      "Successfully installed stopwords-1.0.2\n",
      "Collecting flair\n",
      "  Downloading flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 14.9 MB/s eta 0:00:00\n",
      "Collecting pptree>=3.1\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers[sentencepiece]<5.0.0,>=4.25.0\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "     --------------------------------------- 12.0/12.0 MB 34.4 MB/s eta 0:00:00\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3\n",
      "  Downloading transformer_smaller_training_vocab-0.4.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from flair) (1.2.3)\n",
      "Collecting gdown>=4.4.0\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from flair) (4.67.1)\n",
      "Collecting more-itertools>=8.13.0\n",
      "  Downloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "     ---------------------------------------- 69.7/69.7 kB ? eta 0:00:00\n",
      "Collecting boto3>=1.20.27\n",
      "  Downloading boto3-1.42.17-py3-none-any.whl (140 kB)\n",
      "     ---------------------------------------- 140.6/140.6 kB ? eta 0:00:00\n",
      "Collecting langdetect>=1.0.9\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ------------------------------------- 981.5/981.5 kB 60.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sqlitedict>=2.0.0\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bioc<3.0.0,>=2.0.0\n",
      "  Downloading bioc-2.1-py3-none-any.whl (33 kB)\n",
      "Collecting deprecated>=1.2.13\n",
      "  Downloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting torch>=1.13.1\n",
      "  Downloading torch-2.8.0-cp39-cp39-win_amd64.whl (241.2 MB)\n",
      "     -------------------------------------- 241.2/241.2 MB 9.2 MB/s eta 0:00:00\n",
      "Collecting conllu<5.0.0,>=4.0\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Collecting pytorch-revgrad>=0.2.0\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Collecting wikipedia-api>=0.5.7\n",
      "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2025.11.3-cp39-cp39-win_amd64.whl (277 kB)\n",
      "     ------------------------------------- 277.8/277.8 kB 17.8 MB/s eta 0:00:00\n",
      "Collecting mpld3>=0.3\n",
      "  Downloading mpld3-0.5.12-py3-none-any.whl (203 kB)\n",
      "     -------------------------------------- 203.1/203.1 kB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from flair) (1.6.1)\n",
      "Collecting lxml>=4.8.0\n",
      "  Downloading lxml-6.0.2-cp39-cp39-win_amd64.whl (4.0 MB)\n",
      "     ---------------------------------------- 4.0/4.0 MB 36.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from flair) (3.9.4)\n",
      "Collecting segtok>=1.5.11\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting ftfy>=6.1.0\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 44.8/44.8 kB ? eta 0:00:00\n",
      "Collecting tabulate>=0.8.10\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting intervaltree\n",
      "  Downloading intervaltree-3.2.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting jsonlines>=1.2.0\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.43.0,>=1.42.17\n",
      "  Downloading botocore-1.42.17-py3-none-any.whl (14.6 MB)\n",
      "     --------------------------------------- 14.6/14.6 MB 34.4 MB/s eta 0:00:00\n",
      "Collecting s3transfer<0.17.0,>=0.16.0\n",
      "  Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.8/86.8 kB ? eta 0:00:00\n",
      "Collecting wrapt<3,>=1.10\n",
      "  Downloading wrapt-2.0.1-cp39-cp39-win_amd64.whl (60 kB)\n",
      "     ---------------------------------------- 60.4/60.4 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wcwidth in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from ftfy>=6.1.0->flair) (0.2.5)\n",
      "Requirement already satisfied: requests[socks] in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from gdown>=4.4.0->flair) (2.32.5)\n",
      "Requirement already satisfied: filelock in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from gdown>=4.4.0->flair) (3.19.1)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "     ---------------------------------------- 107.7/107.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (0.28.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (1.2.0)\n",
      "Requirement already satisfied: typer-slim in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.15.0)\n",
      "Requirement already satisfied: shellingham in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (1.5.4)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (21.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (2025.10.0)\n",
      "Requirement already satisfied: six in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.60.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2.0.2)\n",
      "Requirement already satisfied: pillow>=8 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (11.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (6.5.2)\n",
      "Requirement already satisfied: jinja2 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from mpld3>=0.3->flair) (3.1.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.5.3)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 33.5 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 35.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from tqdm>=4.63.0->flair) (0.4.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 34.2 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.10.0\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "     ---------------------------------------- 566.1/566.1 kB ? eta 0:00:00\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "     ---------------------------------------- 341.4/341.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: protobuf in c:\\users\\fatema\\appdata\\roaming\\python\\python39\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (3.19.6)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.2.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 5.6 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "     ---------------------------------------- 144.2/144.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: httpcore==1.* in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.10.0->flair) (1.0.9)\n",
      "Requirement already satisfied: certifi in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.10.0->flair) (2022.9.24)\n",
      "Requirement already satisfied: idna in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.10.0->flair) (3.11)\n",
      "Requirement already satisfied: anyio in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.10.0->flair) (4.12.0)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.10.0->flair) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.2.3->flair) (3.23.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.4.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ------------------------------------- 536.2/536.2 kB 35.1 MB/s eta 0:00:00\n",
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "     ---------------------------------------- 374.9/374.9 kB ? eta 0:00:00\n",
      "Collecting soupsieve>=1.6.1\n",
      "  Downloading soupsieve-2.8.1-py3-none-any.whl (36 kB)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from jinja2->mpld3>=0.3->flair) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.4)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from typer-slim->huggingface-hub>=0.10.0->flair) (8.1.8)\n",
      "Requirement already satisfied: psutil in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from accelerate>=0.26.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.10.0->flair) (1.3.1)\n",
      "Building wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=2498900eb18b1f84cf28b12ec8c3cebeca152ed624645897fee45cf35579f377\n",
      "  Stored in directory: c:\\users\\fatema\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for pptree (setup.py): started\n",
      "  Building wheel for pptree (setup.py): finished with status 'done'\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=0b807cd6915107b70e68fc8706e86ccb6b8269822bdc2ed1ac2bd1555468a1f9\n",
      "  Stored in directory: c:\\users\\fatema\\appdata\\local\\pip\\cache\\wheels\\52\\0e\\51\\514e690004ea9713bc3fdb678d5e2768fcc597d0c3b6a3abd2\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=db1e48b2633230d424153fa658ba0d0bb66175474da7a6fcb0308f012e36a323\n",
      "  Stored in directory: c:\\users\\fatema\\appdata\\local\\pip\\cache\\wheels\\f6\\48\\c4\\942f7a1d556fddd2348cb9ac262f251873dfd8a39afec5678e\n",
      "  Building wheel for wikipedia-api (setup.py): started\n",
      "  Building wheel for wikipedia-api (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=474b4bf6d7bdfc93b9c71d93d0f2fb5116d22d17ec2804c9923651a75da074d6\n",
      "  Stored in directory: c:\\users\\fatema\\appdata\\local\\pip\\cache\\wheels\\3d\\70\\27\\d8096daf5fb5b06a116583a7cc05da1225eb845ab44cc59f56\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e623c52815a54594b313f08814b378881b4bf5e7d52024ee8c5397ff52baae1d\n",
      "  Stored in directory: c:\\users\\fatema\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built langdetect pptree sqlitedict wikipedia-api docopt\n",
      "Installing collected packages: sqlitedict, sortedcontainers, pptree, mpmath, docopt, wrapt, urllib3, tabulate, sympy, soupsieve, sentencepiece, safetensors, regex, PySocks, networkx, more-itertools, lxml, langdetect, jsonlines, jmespath, intervaltree, ftfy, conllu, torch, segtok, deprecated, botocore, bioc, beautifulsoup4, wikipedia-api, s3transfer, pytorch-revgrad, mpld3, huggingface-hub, tokenizers, gdown, boto3, accelerate, transformers, transformer-smaller-training-vocab, flair\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.6.2\n",
      "    Uninstalling urllib3-2.6.2:\n",
      "      Successfully uninstalled urllib3-2.6.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.2.3\n",
      "    Uninstalling huggingface_hub-1.2.3:\n",
      "      Successfully uninstalled huggingface_hub-1.2.3\n",
      "Successfully installed PySocks-1.7.1 accelerate-1.10.1 beautifulsoup4-4.14.3 bioc-2.1 boto3-1.42.17 botocore-1.42.17 conllu-4.5.3 deprecated-1.3.1 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 gdown-5.2.0 huggingface-hub-0.36.0 intervaltree-3.2.1 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 lxml-6.0.2 more-itertools-10.8.0 mpld3-0.5.12 mpmath-1.3.0 networkx-3.2.1 pptree-3.1 pytorch-revgrad-0.2.0 regex-2025.11.3 s3transfer-0.16.0 safetensors-0.7.0 segtok-1.5.11 sentencepiece-0.2.1 sortedcontainers-2.4.0 soupsieve-2.8.1 sqlitedict-2.1.0 sympy-1.14.0 tabulate-0.9.0 tokenizers-0.22.1 torch-2.8.0 transformer-smaller-training-vocab-0.4.2 transformers-4.57.3 urllib3-1.26.20 wikipedia-api-0.8.1 wrapt-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.11.0 requires h5py>=2.9.0, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 19.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: click in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n",
      "Collecting swifter\n",
      "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 12.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=1.0.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from swifter) (2.3.3)\n",
      "Requirement already satisfied: psutil>=5.6.6 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from swifter) (5.9.0)\n",
      "Collecting dask[dataframe]>=2.10.0\n",
      "  Downloading dask-2024.8.0-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.33.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from swifter) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (6.0.3)\n",
      "Collecting toolz>=0.10.0\n",
      "  Downloading toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.1/58.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (8.7.1)\n",
      "Collecting cloudpickle>=1.5.0\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: click>=8.1 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (8.1.8)\n",
      "Collecting partd>=1.4.0\n",
      "  Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (21.3)\n",
      "Collecting dask-expr<1.2,>=1.1\n",
      "  Downloading dask_expr-1.1.10-py3-none-any.whl (242 kB)\n",
      "     ---------------------------------------- 242.2/242.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from pandas>=1.0.0->swifter) (2025.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.0.2)\n",
      "Requirement already satisfied: colorama in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.5)\n",
      "Requirement already satisfied: pyarrow>=7.0.0 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (21.0.0)\n",
      "Requirement already satisfied: zipp>=3.20 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.10.0->swifter) (3.23.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from packaging>=20.0->dask[dataframe]>=2.10.0->swifter) (3.0.9)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in e:\\vs code workspaces\\deeplearningnvidia\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.16.0)\n",
      "Building wheels for collected packages: swifter\n",
      "  Building wheel for swifter (setup.py): started\n",
      "  Building wheel for swifter (setup.py): finished with status 'done'\n",
      "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16510 sha256=88c36e2e30ef813a516066293f949a3d349ddf2601a1cd291089c1c1ddb42ccd\n",
      "  Stored in directory: c:\\users\\fatema\\appdata\\local\\pip\\cache\\wheels\\7b\\4a\\7e\\bcc48cf10e10fcf5b4dae464a66b523756db6b950e02129680\n",
      "Successfully built swifter\n",
      "Installing collected packages: toolz, locket, cloudpickle, partd, dask, dask-expr, swifter\n",
      "Successfully installed cloudpickle-3.1.2 dask-2024.8.0 dask-expr-1.1.10 locket-1.0.0 partd-1.4.2 swifter-1.4.0 toolz-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stopwords\n",
    "!pip install flair\n",
    "!pip install nltk\n",
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd848389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11467d3b",
   "metadata": {},
   "source": [
    "### 1 - Data Acquisition: Import the Dataset from Kaggle\n",
    "- Dataset (Kaggle): [steam-reviews](https://www.kaggle.com/datasets/andrewmvd/steam-reviews/data)\n",
    "\n",
    "- Dataset (local): [data/dataset.csv](data/dataset.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97900716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - Dataset included under data directory\n",
    "# path = kagglehub.dataset_download(\"andrewmvd/steam-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b04b65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Fatema\\\\.cache\\\\kagglehub\\\\datasets\\\\andrewmvd\\\\steam-reviews\\\\versions\\\\3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional \n",
    "# path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d64fbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) copied.\n"
     ]
    }
   ],
   "source": [
    "# Optional: Dataset attached in repository\n",
    "# Do change the source path when running on another machine using the output of the cell above\n",
    "# ! copy C:\\Users\\Fatema\\.cache\\kagglehub\\datasets\\andrewmvd\\steam-reviews\\versions\\3\\dataset.csv data\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc77142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Ruined my life.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>This will be more of a ''my experience with th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>This game saved my virginity.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>• Do you like original games? • Do you like ga...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Easy to learn, hard to master.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id        app_name                                        review_text  \\\n",
       "0      10  Counter-Strike                                    Ruined my life.   \n",
       "1      10  Counter-Strike  This will be more of a ''my experience with th...   \n",
       "2      10  Counter-Strike                      This game saved my virginity.   \n",
       "3      10  Counter-Strike  • Do you like original games? • Do you like ga...   \n",
       "4      10  Counter-Strike           Easy to learn, hard to master.             \n",
       "\n",
       "   review_score  review_votes  \n",
       "0             1             0  \n",
       "1             1             1  \n",
       "2             1             0  \n",
       "3             1             0  \n",
       "4             1             1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d518cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6417106 entries, 0 to 6417105\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   app_id        int64 \n",
      " 1   app_name      object\n",
      " 2   review_text   object\n",
      " 3   review_score  int64 \n",
      " 4   review_votes  int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 244.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb3c3dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.417106e+06</td>\n",
       "      <td>6.417106e+06</td>\n",
       "      <td>6.417106e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.274695e+05</td>\n",
       "      <td>6.394992e-01</td>\n",
       "      <td>1.472446e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.260451e+05</td>\n",
       "      <td>7.687918e-01</td>\n",
       "      <td>3.543496e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.018100e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.391600e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.056200e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.653400e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             app_id  review_score  review_votes\n",
       "count  6.417106e+06  6.417106e+06  6.417106e+06\n",
       "mean   2.274695e+05  6.394992e-01  1.472446e-01\n",
       "std    1.260451e+05  7.687918e-01  3.543496e-01\n",
       "min    1.000000e+01 -1.000000e+00  0.000000e+00\n",
       "25%    2.018100e+05  1.000000e+00  0.000000e+00\n",
       "50%    2.391600e+05  1.000000e+00  0.000000e+00\n",
       "75%    3.056200e+05  1.000000e+00  0.000000e+00\n",
       "max    5.653400e+05  1.000000e+00  1.000000e+00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1bc39b",
   "metadata": {},
   "source": [
    "### 2- EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "333db436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games reviewed: 9972\n"
     ]
    }
   ],
   "source": [
    "print('Number of games reviewed: {}'.format(df['app_id'].nunique())) # how many games are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "576c3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Sentiment (review_score) class balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc817a11",
   "metadata": {},
   "source": [
    "### 3- Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d075fe6",
   "metadata": {},
   "source": [
    "### 4- Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d48e7",
   "metadata": {},
   "source": [
    "### 5- Results Evaluations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
